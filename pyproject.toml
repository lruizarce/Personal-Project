[project]
name = "llm-response-service"
version = "0.1.0"
description = "A FastAPI service that provides a unified API for serving LLM responses, with conversation persistence, streaming support, prompt template management, and usage tracking."
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "alembic>=1.17.2",
    "anthropic>=0.75.0",
    "asyncpg>=0.31.0",
    "fastapi[standard]>=0.124.0",
    "httpx>=0.28.1",
    "openai>=2.9.0",
    "pydantic>=2.12.5",
    "pydantic-settings>=2.12.0",
    "python-jose>=3.5.0",
    "redis>=7.1.0",
    "sqlalchemy[asyncio]>=2.0.44",
    "sse-starlette>=3.0.3",
    "tiktoken>=0.12.0",
    "uvicorn[standard]>=0.38.0",
]
