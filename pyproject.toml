[project]
name = "llm-response-service"
version = "0.1.0"
description = "A FastAPI service that provides a unified API for serving LLM responses, with conversation persistence, streaming support, prompt template management, and usage tracking."
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "anthropic>=0.75.0",
    "fastapi[standard]>=0.124.0",
    "httpx>=0.28.1",
    "pydantic>=2.12.5",
    "pydantic-settings>=2.12.0",
    "sqlalchemy>=2.0.44",
    "uvicorn[standard]>=0.38.0",
]
